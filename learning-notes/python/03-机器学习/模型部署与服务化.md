# 模型部署与服务化

> 机器学习模型部署和服务化实践

## 1. FastAPI 模型服务

### 1.1 基础服务

```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI()

# 加载模型
model = joblib.load('model.pkl')

class PredictionRequest(BaseModel):
    features: list[float]

class PredictionResponse(BaseModel):
    prediction: float
    probability: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    features = np.array(request.features).reshape(1, -1)
    prediction = model.predict(features)[0]
    probability = model.predict_proba(features)[0].max()
    
    return PredictionResponse(
        prediction=float(prediction),
        probability=float(probability)
    )
```

### 1.2 模型版本管理

```python
from fastapi import FastAPI, Header
from typing import Optional

app = FastAPI()

models = {
    'v1': joblib.load('model_v1.pkl'),
    'v2': joblib.load('model_v2.pkl')
}

@app.post("/predict")
async def predict(
    request: PredictionRequest,
    model_version: Optional[str] = Header("v2", alias="X-Model-Version")
):
    model = models.get(model_version, models['v2'])
    prediction = model.predict(np.array(request.features).reshape(1, -1))
    return {"prediction": float(prediction[0])}
```

## 2. TensorFlow Serving

### 2.1 模型导出

```python
import tensorflow as tf

# 保存模型为SavedModel格式
model.save('saved_model/my_model', save_format='tf')

# 或使用tf.saved_model.save
tf.saved_model.save(model, 'saved_model/my_model')
```

### 2.2 Docker 部署

```dockerfile
FROM tensorflow/serving

COPY saved_model/my_model /models/my_model
ENV MODEL_NAME=my_model

CMD ["tensorflow_model_server"]
```

### 2.3 Python 客户端

```python
import grpc
import tensorflow as tf
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc

channel = grpc.insecure_channel('localhost:8500')
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

request = predict_pb2.PredictRequest()
request.model_spec.name = 'my_model'
request.model_spec.signature_name = 'serving_default'
request.inputs['input'].CopyFrom(tf.make_tensor_proto(features))

result = stub.Predict(request)
```

## 3. TorchServe

### 3.1 模型打包

```python
import torch
import torchvision.models as models

# 加载模型
model = models.resnet50(pretrained=True)
model.eval()

# 保存模型
torch.save(model.state_dict(), 'model.pth')

# 创建模型处理器
# model_handler.py
def handle(data, context):
    # 处理请求
    pass
```

### 3.2 部署

```bash
# 打包模型
torch-model-archiver \
  --model-name resnet50 \
  --version 1.0 \
  --model-file model.py \
  --serialized-file model.pth \
  --handler model_handler.py

# 启动服务
torchserve --start --model-store model_store --models resnet50
```

## 4. MLflow 模型服务

### 4.1 模型注册

```python
import mlflow
import mlflow.sklearn

# 训练模型
model = train_model()

# 记录模型
with mlflow.start_run():
    mlflow.sklearn.log_model(model, "model")
    mlflow.log_params(params)
    mlflow.log_metrics(metrics)

# 注册模型
mlflow.register_model("runs:/run_id/model", "MyModel")
```

### 4.2 模型服务

```python
# 加载模型
model = mlflow.pyfunc.load_model("models:/MyModel/Production")

# 预测
predictions = model.predict(data)
```

## 5. 模型监控

### 5.1 性能监控

```python
from prometheus_client import Counter, Histogram
import time

prediction_count = Counter('model_predictions_total', 'Total predictions')
prediction_latency = Histogram('model_prediction_latency_seconds', 'Prediction latency')

@app.post("/predict")
async def predict(request: PredictionRequest):
    start_time = time.time()
    
    try:
        prediction = model.predict(request.features)
        prediction_count.inc()
        return {"prediction": prediction}
    except Exception as e:
        prediction_count.inc({'status': 'error'})
        raise
    finally:
        prediction_latency.observe(time.time() - start_time)
```

### 5.2 A/B 测试

```python
import random

models = {
    'control': load_model('model_v1.pkl'),
    'treatment': load_model('model_v2.pkl')
}

@app.post("/predict")
async def predict(request: PredictionRequest, user_id: int):
    # 根据用户ID分配模型
    variant = 'treatment' if user_id % 2 == 0 else 'control'
    model = models[variant]
    
    prediction = model.predict(request.features)
    
    # 记录实验数据
    log_experiment(user_id, variant, prediction)
    
    return {"prediction": prediction, "variant": variant}
```

## 6. 批量预测

### 6.1 异步处理

```python
from celery import Celery

celery_app = Celery('tasks', broker='redis://localhost:6379')

@celery_app.task
def batch_predict(data):
    predictions = model.predict(data)
    return predictions.tolist()

@app.post("/predict/batch")
async def batch_predict_endpoint(request: BatchRequest):
    task = batch_predict.delay(request.data)
    return {"task_id": task.id}

@app.get("/predict/batch/{task_id}")
async def get_batch_result(task_id: str):
    task = batch_predict.AsyncResult(task_id)
    if task.ready():
        return {"status": "completed", "result": task.result}
    return {"status": "pending"}
```

## 7. 总结

模型部署与服务化要点：
- **FastAPI服务**：基础服务、模型版本管理
- **TensorFlow Serving**：模型导出、Docker部署、gRPC客户端
- **TorchServe**：模型打包、服务启动
- **MLflow**：模型注册、模型服务
- **模型监控**：性能监控、A/B测试
- **批量预测**：异步处理、任务队列

模型服务化实现生产环境部署。

