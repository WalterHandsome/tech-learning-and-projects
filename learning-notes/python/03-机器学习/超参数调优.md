# 超参数调优

> 机器学习模型超参数优化方法

## 1. 网格搜索

### 1.1 GridSearchCV

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

model = RandomForestClassifier()
grid_search = GridSearchCV(
    model, param_grid, cv=5, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X_train, y_train)

print(f"最佳参数: {grid_search.best_params_}")
print(f"最佳分数: {grid_search.best_score_}")
```

## 2. 随机搜索

### 2.1 RandomizedSearchCV

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_distributions = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(3, 10),
    'min_samples_split': [2, 5, 10],
    'max_features': uniform(0.1, 0.9)
}

random_search = RandomizedSearchCV(
    model, param_distributions, n_iter=100, cv=5, 
    scoring='accuracy', n_jobs=-1, random_state=42
)
random_search.fit(X_train, y_train)
```

## 3. 贝叶斯优化

### 3.1 Optuna

```python
import optuna
from sklearn.model_selection import cross_val_score

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'max_features': trial.suggest_float('max_features', 0.1, 1.0)
    }
    
    model = RandomForestClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=5)
    return scores.mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"最佳参数: {study.best_params}")
print(f"最佳分数: {study.best_value}")
```

## 4. 超参数重要性

### 4.1 参数重要性分析

```python
import optuna.visualization as vis

# 参数重要性
fig = vis.plot_param_importances(study)
fig.show()

# 优化历史
fig = vis.plot_optimization_history(study)
fig.show()

# 参数关系
fig = vis.plot_parallel_coordinate(study)
fig.show()
```

## 5. 早停策略

### 5.1 早停回调

```python
import optuna

class EarlyStoppingCallback:
    def __init__(self, patience=10):
        self.patience = patience
        self.best_score = None
        self.no_improve_count = 0
    
    def __call__(self, study, trial):
        if study.best_value is None:
            return
        
        if self.best_score is None or study.best_value > self.best_score:
            self.best_score = study.best_value
            self.no_improve_count = 0
        else:
            self.no_improve_count += 1
        
        if self.no_improve_count >= self.patience:
            study.stop()

study = optuna.create_study(direction='maximize')
study.optimize(
    objective, 
    n_trials=100,
    callbacks=[EarlyStoppingCallback(patience=10)]
)
```

## 6. 深度学习超参数调优

### 6.1 Keras Tuner

```python
from tensorflow import keras
from tensorflow.keras import layers
import kerastuner as kt

def build_model(hp):
    model = keras.Sequential()
    model.add(layers.Dense(
        units=hp.Int('units', min_value=32, max_value=512, step=32),
        activation='relu'
    ))
    model.add(layers.Dropout(
        rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)
    ))
    model.add(layers.Dense(1, activation='sigmoid'))
    
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
        ),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=3
)

tuner.search(X_train, y_train, epochs=10, validation_split=0.2)
best_model = tuner.get_best_models(num_models=1)[0]
```

## 7. 总结

超参数调优要点：
- **网格搜索**：穷举搜索、GridSearchCV
- **随机搜索**：随机采样、RandomizedSearchCV
- **贝叶斯优化**：Optuna、智能搜索
- **参数重要性**：可视化分析
- **早停策略**：避免无效搜索
- **深度学习调优**：Keras Tuner

超参数调优提高模型性能。

