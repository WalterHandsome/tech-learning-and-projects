# 爬虫框架深入

> Python 爬虫框架的高级应用和最佳实践

## 1. Scrapy 深入

### 1.1 项目结构

```python
# scrapy.cfg
[settings]
default = myproject.settings

# myproject/spiders/example.py
import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['http://example.com']
    
    def parse(self, response):
        for item in response.css('div.item'):
            yield {
                'title': item.css('h2::text').get(),
                'link': item.css('a::attr(href)').get()
            }
        
        # 跟进链接
        next_page = response.css('a.next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
```

### 1.2 Item Pipeline

```python
# pipelines.py
class MyPipeline:
    def process_item(self, item, spider):
        # 数据清洗
        item['title'] = item['title'].strip()
        
        # 数据验证
        if not item.get('title'):
            raise DropItem("缺少标题")
        
        # 数据存储
        save_to_database(item)
        return item
```

### 1.3 中间件

```python
# middlewares.py
class CustomMiddleware:
    def process_request(self, request, spider):
        # 添加请求头
        request.headers['User-Agent'] = 'Custom Agent'
        return None
    
    def process_response(self, request, response, spider):
        # 处理响应
        if response.status == 403:
            return request.replace(dont_filter=True)
        return response
```

## 2. Scrapy-Redis 分布式

### 2.1 配置

```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
REDIS_URL = 'redis://localhost:6379'
```

### 2.2 分布式爬虫

```python
from scrapy_redis.spiders import RedisSpider

class DistributedSpider(RedisSpider):
    name = 'distributed'
    redis_key = 'spider:start_urls'
    
    def parse(self, response):
        # 解析逻辑
        pass
```

## 3. Selenium 高级应用

### 3.1 无头浏览器

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)
driver.get('http://example.com')
```

### 3.2 等待策略

```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

wait = WebDriverWait(driver, 10)
element = wait.until(
    EC.presence_of_element_located((By.ID, "myElement"))
)
```

### 3.3 反检测

```python
options = Options()
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)

driver = webdriver.Chrome(options=options)
driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
    'source': '''
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined
        })
    '''
})
```

## 4. Playwright 应用

### 4.1 基础使用

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.new_page()
    page.goto('http://example.com')
    
    # 等待元素
    page.wait_for_selector('div.content')
    
    # 获取内容
    content = page.inner_text('div.content')
    
    browser.close()
```

### 4.2 异步爬虫

```python
from playwright.async_api import async_playwright

async def crawl():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto('http://example.com')
        
        # 异步操作
        content = await page.inner_text('div.content')
        
        await browser.close()

import asyncio
asyncio.run(crawl())
```

## 5. 数据解析技巧

### 5.1 XPath 高级用法

```python
# 相对路径
items = response.xpath('//div[@class="item"]')

# 属性选择
links = response.xpath('//a[@href]')

# 文本提取
text = response.xpath('//div/text()').get()

# 多条件
items = response.xpath('//div[@class="item" and @id]')
```

### 5.2 CSS 选择器

```python
# 类选择
items = response.css('div.item')

# 属性选择
links = response.css('a[href]')

# 伪类
first_item = response.css('div.item:first-child')

# 组合选择
content = response.css('div.content > p::text')
```

## 6. 总结

爬虫框架深入要点：
- **Scrapy深入**：项目结构、Pipeline、中间件
- **分布式爬虫**：Scrapy-Redis、分布式调度
- **Selenium高级**：无头浏览器、等待策略、反检测
- **Playwright**：同步/异步、现代浏览器自动化
- **数据解析**：XPath、CSS选择器高级用法

深入掌握爬虫框架提高爬取效率。

