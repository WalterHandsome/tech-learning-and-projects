# 数据存储与管理

> 爬虫数据的存储和管理策略

## 1. 数据存储方案

### 1.1 文件存储

```python
import json
import csv

# JSON存储
def save_json(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# CSV存储
def save_csv(data, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)
```

### 1.2 数据库存储

```python
import sqlite3
import pymongo

# SQLite存储
def save_to_sqlite(data, db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS items (
            id INTEGER PRIMARY KEY,
            title TEXT,
            url TEXT,
            content TEXT
        )
    ''')
    
    for item in data:
        cursor.execute('INSERT INTO items (title, url, content) VALUES (?, ?, ?)',
                      (item['title'], item['url'], item['content']))
    
    conn.commit()
    conn.close()

# MongoDB存储
def save_to_mongodb(data, db_name, collection_name):
    client = pymongo.MongoClient('mongodb://localhost:27017/')
    db = client[db_name]
    collection = db[collection_name]
    collection.insert_many(data)
```

## 2. 数据去重

### 2.1 基于URL去重

```python
import hashlib

class URLManager:
    def __init__(self):
        self.visited_urls = set()
    
    def add_url(self, url):
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if url_hash not in self.visited_urls:
            self.visited_urls.add(url_hash)
            return True
        return False
```

### 2.2 基于内容去重

```python
def content_hash(content):
    return hashlib.md5(content.encode()).hexdigest()

def is_duplicate(content, seen_hashes):
    hash_value = content_hash(content)
    if hash_value in seen_hashes:
        return True
    seen_hashes.add(hash_value)
    return False
```

## 3. 增量爬取

### 3.1 基于时间戳

```python
import time
from datetime import datetime

class IncrementalCrawler:
    def __init__(self, last_crawl_time_file='last_crawl.txt'):
        self.last_crawl_time_file = last_crawl_time_file
        self.last_crawl_time = self.load_last_crawl_time()
    
    def load_last_crawl_time(self):
        try:
            with open(self.last_crawl_time_file, 'r') as f:
                return float(f.read())
        except:
            return 0
    
    def save_last_crawl_time(self):
        with open(self.last_crawl_time_file, 'w') as f:
            f.write(str(time.time()))
    
    def should_crawl(self, item_time):
        return item_time > self.last_crawl_time
```

## 4. 数据清洗

### 4.1 文本清洗

```python
import re
from bs4 import BeautifulSoup

def clean_text(text):
    # 移除HTML标签
    soup = BeautifulSoup(text, 'html.parser')
    text = soup.get_text()
    
    # 移除多余空白
    text = re.sub(r'\s+', ' ', text)
    
    # 移除特殊字符
    text = re.sub(r'[^\w\s]', '', text)
    
    return text.strip()
```

## 5. 数据验证

### 5.1 数据完整性检查

```python
def validate_data(item, required_fields):
    for field in required_fields:
        if field not in item or not item[field]:
            return False, f"缺少字段: {field}"
    return True, "数据有效"
```

## 6. 总结

数据存储与管理要点：
- **存储方案**：文件、数据库（SQLite、MongoDB）
- **去重策略**：URL去重、内容去重
- **增量爬取**：基于时间戳、版本号
- **数据清洗**：文本处理、格式统一
- **数据验证**：完整性检查、格式验证

良好的数据管理确保爬虫数据的质量和可用性。

