# 分布式爬虫架构

> 构建可扩展的分布式爬虫系统

## 1. 分布式架构设计

### 1.1 架构组件

```
分布式爬虫架构
├── 调度器（Scheduler）
│   ├── URL队列管理
│   └── 任务分发
├── 爬虫节点（Workers）
│   ├── 网页抓取
│   └── 数据解析
├── 数据存储（Storage）
│   ├── 数据库
│   └── 文件存储
└── 监控系统（Monitor）
    ├── 性能监控
    └── 错误告警
```

## 2. 使用 Redis 作为队列

### 2.1 URL队列管理

```python
import redis
import json

class URLQueue:
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)
        self.queue_key = 'url_queue'
        self.visited_key = 'visited_urls'
    
    def add_url(self, url, priority=0):
        """添加URL到队列"""
        if not self.is_visited(url):
            self.redis_client.zadd(
                self.queue_key,
                {json.dumps({'url': url, 'priority': priority}): priority}
            )
    
    def get_url(self):
        """获取URL"""
        result = self.redis_client.zpopmax(self.queue_key)
        if result:
            return json.loads(result[0][0])
        return None
    
    def mark_visited(self, url):
        """标记URL已访问"""
        self.redis_client.sadd(self.visited_key, url)
    
    def is_visited(self, url):
        """检查URL是否已访问"""
        return self.redis_client.sismember(self.visited_key, url)
```

## 3. 使用 Celery 分布式任务

### 3.1 爬虫任务

```python
from celery import Celery

app = Celery('crawler', broker='redis://localhost:6379/0')

@app.task
def crawl_page(url):
    """爬取单个页面"""
    try:
        response = requests.get(url)
        data = parse_page(response.text)
        save_data(data)
        # 提取新URL
        new_urls = extract_urls(response.text)
        for new_url in new_urls:
            crawl_page.delay(new_url)  # 异步添加新任务
    except Exception as e:
        logger.error(f"爬取失败 {url}: {e}")
```

## 4. 使用 Scrapy 分布式

### 4.1 Scrapy-Redis 配置

```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
REDIS_URL = 'redis://localhost:6379'

# 分布式爬虫
class DistributedSpider(scrapy.Spider):
    name = 'distributed'
    redis_key = 'spider:start_urls'
    
    def parse(self, response):
        # 解析逻辑
        pass
```

## 5. 数据去重

### 5.1 布隆过滤器

```python
from pybloom_live import BloomFilter

bf = BloomFilter(capacity=1000000, error_rate=0.001)

def is_duplicate(url):
    if url in bf:
        return True
    bf.add(url)
    return False
```

## 6. 总结

分布式爬虫要点：
- **队列管理**：Redis作为URL队列
- **任务分发**：Celery分布式任务
- **去重机制**：布隆过滤器
- **数据存储**：分布式存储
- **监控告警**：系统监控

分布式架构可以大幅提升爬虫效率和可扩展性。

