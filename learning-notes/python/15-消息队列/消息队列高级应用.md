# 消息队列高级应用

> 消息队列在生产环境中的高级用法和最佳实践

## 1. RabbitMQ 高级特性

### 1.1 消息确认机制

```python
import pika

def callback(ch, method, properties, body):
    print(f"收到消息: {body.decode()}")
    # 处理消息
    try:
        process_message(body)
        # 手动确认
        ch.basic_ack(delivery_tag=method.delivery_tag)
    except Exception as e:
        # 拒绝消息并重新入队
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

channel.basic_consume(
    queue='task_queue',
    on_message_callback=callback,
    auto_ack=False  # 关闭自动确认
)
```

### 1.2 消息持久化

```python
# 声明持久化队列
channel.queue_declare(queue='durable_queue', durable=True)

# 发送持久化消息
channel.basic_publish(
    exchange='',
    routing_key='durable_queue',
    body='持久化消息',
    properties=pika.BasicProperties(
        delivery_mode=2,  # 消息持久化
    )
)
```

### 1.3 公平分发

```python
# 设置预取数量
channel.basic_qos(prefetch_count=1)

channel.basic_consume(
    queue='task_queue',
    on_message_callback=callback
)
```

### 1.4 发布订阅模式

```python
# 生产者
channel.exchange_declare(exchange='logs', exchange_type='fanout')
channel.basic_publish(exchange='logs', routing_key='', body='消息')

# 消费者
result = channel.queue_declare(queue='', exclusive=True)
queue_name = result.method.queue
channel.queue_bind(exchange='logs', queue=queue_name)
```

### 1.5 路由模式

```python
# 声明直连交换机
channel.exchange_declare(exchange='direct_logs', exchange_type='direct')

# 发送消息
channel.basic_publish(
    exchange='direct_logs',
    routing_key='error',  # 路由键
    body='错误消息'
)

# 绑定队列
channel.queue_bind(
    exchange='direct_logs',
    queue=queue_name,
    routing_key='error'
)
```

### 1.6 主题模式

```python
# 声明主题交换机
channel.exchange_declare(exchange='topic_logs', exchange_type='topic')

# 发送消息
channel.basic_publish(
    exchange='topic_logs',
    routing_key='user.created',  # 主题路由键
    body='用户创建消息'
)

# 绑定（支持通配符）
channel.queue_bind(
    exchange='topic_logs',
    queue=queue_name,
    routing_key='user.*'  # 匹配 user. 开头的所有消息
)
```

## 2. Kafka 高级应用

### 2.1 生产者配置

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    key_serializer=lambda k: k.encode('utf-8') if k else None,
    acks='all',  # 等待所有副本确认
    retries=3,
    max_in_flight_requests_per_connection=1,
    enable_idempotence=True  # 幂等性
)

# 发送消息
future = producer.send('my_topic', key='key', value={'data': 'value'})
record_metadata = future.get(timeout=10)
```

### 2.2 消费者组

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'my_topic',
    bootstrap_servers=['localhost:9092'],
    group_id='my_group',
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    auto_offset_reset='earliest',  # 从最早开始
    enable_auto_commit=False  # 手动提交
)

for message in consumer:
    print(f"分区: {message.partition}, 偏移量: {message.offset}")
    print(f"消息: {message.value}")
    # 处理消息
    process_message(message.value)
    # 手动提交偏移量
    consumer.commit()
```

### 2.3 分区和副本

```python
from kafka.admin import KafkaAdminClient, NewTopic

admin_client = KafkaAdminClient(
    bootstrap_servers=['localhost:9092']
)

topic = NewTopic(
    name='my_topic',
    num_partitions=3,  # 3个分区
    replication_factor=2  # 2个副本
)

admin_client.create_topics([topic])
```

## 3. Redis 消息队列

### 3.1 列表队列

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

# 生产者
r.lpush('task_queue', 'task1')
r.lpush('task_queue', 'task2')

# 消费者（阻塞）
while True:
    task = r.brpop('task_queue', timeout=10)
    if task:
        process_task(task[1].decode())
```

### 3.2 发布订阅

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)
pubsub = r.pubsub()

# 订阅频道
pubsub.subscribe('channel1', 'channel2')

# 监听消息
for message in pubsub.listen():
    if message['type'] == 'message':
        print(f"收到消息: {message['data'].decode()}")

# 发布消息
r.publish('channel1', 'Hello, Redis!')
```

### 3.3 Stream（Redis 5.0+）

```python
# 生产者
r.xadd('mystream', {'field1': 'value1', 'field2': 'value2'})

# 消费者组
r.xgroup_create('mystream', 'mygroup', id='0', mkstream=True)

# 读取消息
messages = r.xreadgroup(
    'mygroup', 'consumer1',
    {'mystream': '>'},
    count=10,
    block=1000
)
```

## 4. Celery 分布式任务

### 4.1 任务定义

```python
from celery import Celery

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task(bind=True, max_retries=3)
def process_task(self, data):
    try:
        # 处理任务
        result = do_work(data)
        return result
    except Exception as exc:
        # 重试
        raise self.retry(exc=exc, countdown=60)

@app.task
def send_email(to, subject, body):
    # 发送邮件
    pass
```

### 4.2 任务路由

```python
# celeryconfig.py
task_routes = {
    'tasks.send_email': {'queue': 'email'},
    'tasks.process_data': {'queue': 'data'},
}

# 启动worker
# celery -A tasks worker -Q email,data
```

### 4.3 定时任务

```python
from celery.schedules import crontab

app.conf.beat_schedule = {
    'send-daily-report': {
        'task': 'tasks.send_report',
        'schedule': crontab(hour=9, minute=0),  # 每天9点
    },
    'cleanup-temp-files': {
        'task': 'tasks.cleanup',
        'schedule': 3600.0,  # 每小时
    },
}
```

## 5. 消息队列最佳实践

### 5.1 消息幂等性

```python
import hashlib
import redis

def is_processed(message_id):
    """检查消息是否已处理"""
    r = redis.Redis()
    key = f"processed:{message_id}"
    return r.exists(key)

def mark_processed(message_id):
    """标记消息已处理"""
    r = redis.Redis()
    key = f"processed:{message_id}"
    r.setex(key, 86400, '1')  # 24小时过期

def process_with_idempotency(message):
    message_id = hashlib.md5(str(message).encode()).hexdigest()
    
    if is_processed(message_id):
        print("消息已处理，跳过")
        return
    
    # 处理消息
    do_process(message)
    mark_processed(message_id)
```

### 5.2 死信队列

```python
# RabbitMQ死信队列配置
channel.queue_declare(
    queue='main_queue',
    arguments={
        'x-dead-letter-exchange': 'dlx',
        'x-dead-letter-routing-key': 'dlq',
        'x-message-ttl': 60000  # 消息TTL
    }
)

# 死信队列
channel.exchange_declare(exchange='dlx', exchange_type='direct')
channel.queue_declare(queue='dlq')
channel.queue_bind(exchange='dlx', queue='dlq', routing_key='dlq')
```

### 5.3 消息重试机制

```python
def process_with_retry(message, max_retries=3):
    for attempt in range(max_retries):
        try:
            return process_message(message)
        except Exception as e:
            if attempt == max_retries - 1:
                # 发送到死信队列
                send_to_dlq(message, str(e))
                raise
            time.sleep(2 ** attempt)  # 指数退避
```

### 5.4 消息顺序保证

```python
# 使用单分区保证顺序（Kafka）
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    partitioner=lambda key, all_partitions, available: 0  # 固定分区
)

# 或使用消息键保证同一键的消息在同一分区
producer.send('topic', key='user_id', value=data)
```

## 6. 监控和告警

### 6.1 消息队列监控

```python
import pika

def get_queue_info(channel, queue_name):
    method = channel.queue_declare(queue=queue_name, passive=True)
    return {
        'queue': queue_name,
        'messages': method.method.message_count,
        'consumers': method.method.consumer_count
    }

# 监控队列长度
queue_info = get_queue_info(channel, 'task_queue')
if queue_info['messages'] > 1000:
    send_alert("队列积压过多")
```

## 7. 总结

消息队列高级应用要点：
- **消息确认**：确保消息可靠处理
- **持久化**：防止消息丢失
- **路由模式**：灵活的消息分发
- **消费者组**：负载均衡和容错
- **幂等性**：防止重复处理
- **死信队列**：处理失败消息
- **重试机制**：提高可靠性
- **监控告警**：及时发现问题

合理使用这些高级特性，可以构建可靠的消息队列系统。

